{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Text Mining\n",
    "- Find any suitable textual data for processing which will contain at least 500 sentences.\n",
    "you can manually collect texts from BBC/CNN/New York Times, or\n",
    "use the crawler from the first homework/tutorial and extend it to crawl particular website and collect content for this task, or\n",
    "use any other suitable texts (e.g. OpenData speech datasets)\n",
    "- Perform following NLP tasks:\n",
    "- POS tagging\n",
    "- NER with entity classification (using nltk.ne_chunk)\n",
    "- NER with custom patterns\n",
    "\n",
    "e.g. every match of: adjective (optional) and proper noun (singular/plural) is matched as the entity\n",
    "see slides 31 or 38 from lecture 4 for some NLTK examples using RegexpParser or custom NER\n",
    "- Implement your custom entity classification for each detected entity (using both nltk.ne_chunk and custom patterns)\n",
    "- Try to find a page in the Wikipedia\n",
    "- Extract the first sentence from the summary\n",
    "Detect category from the sentence as a noun phrase\n",
    "Example:\n",
    "for „Wikipedia“ entity the first sentence is „Wikipedia (/ˌwɪkᵻˈpiːdiə/ or /ˌwɪkiˈpiːdiə/ WIK-i-PEE-dee-ə) is a free online encyclopedia that aims to allow anyone to edit articles.“\n",
    "you can detect pattern „…​ is/VBZ a/DT free/JJ online/NN encyclopedia/NN …​“\n",
    "the output can be „Wikipedia“: „free online encyklopedia“\n",
    "For unknown entities assign default category e.g. „Thing“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import nltk\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task execution\n",
    "I chose the book Silmarillion for this task as the text is long enough and contains many non-english terms and names.\n",
    "\n",
    "#### Contents of **src** folder:\n",
    "  - jupyter notebook\n",
    "  - silmarillion.txt  (input text)\n",
    "\n",
    "#### Contents of **res** folder: \n",
    "  - entities_binary.json (NER with nltk.ne_chunks())\n",
    "  - entities_all.json (NER with nltk.ne_chunks())\n",
    "  - entities_custom.json (NER with custom pattern)\n",
    "  - named_entitites_wordbook.txt (Wikipedia categorization of NE with nltk entities (binary))\n",
    "  - custom.txt (Wikipedia categorization of NE with custom entities)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text file (Silmarillion) into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  711475\n",
      "Number of sentences:  4207\n",
      "\n",
      " AINU LIN DALE \n",
      "\n",
      "\n",
      "The Music of the Ainur \n",
      "\n",
      "\n",
      "There was Eru, the One, who in Arda is called Iluvatar; \n",
      "and he made first the Ainur, the Holy Ones, that were the \n",
      "offspring of his thought, and they were with him before \n",
      "aught else was made. And he spoke  ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads text file into string\n",
    "\"\"\"\n",
    "def read_book(name):\n",
    "    book = \"\"\n",
    "    with open(name, 'r') as file:\n",
    "        book = file.read()\n",
    "    file.close()\n",
    "    return book\n",
    "\n",
    "## Loading whole Silmarillion book by Tolkien (apart from foreword, appendix, pronounciation notes and name index)\n",
    "text = read_book('./../src/silmarillion.txt')\n",
    "\n",
    "text = text.replace('lluvatar', 'Iluvatar') # fixing possible transcription error\n",
    "text = text.replace('Copyright', \"\")\n",
    "\n",
    "print(\"Number of characters: \", len(text))\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(\"Number of sentences: \", len(sentences))\n",
    "\n",
    "print(\"\\n\",text[:250], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First couple of tagged tokens: \n",
      "[('AINU', 'NNP'), ('LIN', 'NNP'), ('DALE', 'NNP'), ('The', 'DT'), ('Music', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Ainur', 'NNP'), ('There', 'EX'), ('was', 'VBD'), ('Eru', 'NNP'), (',', ','), ('the', 'DT'), ('One', 'CD'), (',', ','), ('who', 'WP'), ('in', 'IN'), ('Arda', 'NNP'), ('is', 'VBZ'), ('called', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "## Creating list of tokens from string\n",
    "tokenz =  word_tokenize(text)\n",
    "\n",
    "## Tagging tokens with their corresponding part of speech categories\n",
    "tagged = pos_tag(tokenz)\n",
    "\n",
    "print(\"First couple of tagged tokens: \")\n",
    "print(tagged[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER chunks with nltk.ne_chunks()\n",
    "Classification (recognition) of Named Entities from POS-tagged tokens\n",
    " - binary = True : only recognizes if token is NE or not\n",
    " - binary = False : tries to recognize type of NE (i.e. person, location) ... this setting seems to be splitting some entities with more than 1 word and does not work wery well, given that those mostly made-up words and names in any Tolkien book must be harder to classify it's unsurprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function from 2nd tutorial -> from list of tagged words extracts those recognized as named entities\n",
    "\"\"\"\n",
    "def extractEntities(chunks):\n",
    "    data = {}\n",
    "    for entity in chunks:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE with diff. types \n",
    "ne_chunked_f = ne_chunk(tagged, binary=False)\n",
    "ne_entities_f = extractEntities(ne_chunked_f)\n",
    "\n",
    "# NE without further classification\n",
    "ne_chunked = ne_chunk(tagged, binary=True)\n",
    "ne_entities = extractEntities(ne_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Further classified NE: \n",
      "['Aredhel Turgon', 'Nienna', 'Colon', 'Rivil', 'Finrod Finarfin', 'Curse', 'Hallow', 'Emyn Beraid', 'Darkness', 'Crissaegrim', 'Orthanc', 'ButAtanamir', 'Younger', 'Nay', 'Borlad', 'Erech', 'Elder Children', 'Serinde', 'Mount Rerir', 'High Hope', 'Dimbar', 'Anarion', 'White Telperion', 'Eledhwen', 'Gwindor', 'Dark Years', 'Finrod', 'Tuneless Halls', 'Thangorodrim', 'Earendur', 'Nan Elmoth', 'Valiant', 'Long', 'Haidar', 'Wolf', 'Uruloki', 'Blessed Realm', 'Grey Havens', 'Nen Girith', 'Immortal', 'Tol Galen', 'Teiglin Turin', 'Denethor', 'Winged Crown', 'Fear', 'Seven Stars', 'Kingdom', 'Eldalie', 'Kindler', 'Fingon Fingolfin']\n",
      "Only NE: \n",
      "['Amon Amarth', 'Nienna', 'Twilight Meres', 'Colon', 'Rivil', 'Finrod Finarfin', 'Hallow', 'Emyn Beraid', 'Darkness', 'Crissaegrim', 'Orthanc', 'Younger', 'Annael', 'Borlad', 'Erech', 'Elder Children', 'Serinde', 'Mount Rerir', 'Silmarien', 'Dimbar', 'Anarion', 'White Telperion', 'Eledhwen', 'Gwindor', 'Dark Years', 'Finrod', 'Tuneless Halls', 'Thangorodrim', 'Nan Elmoth', 'Valiant', 'Long', 'Haidar', 'Wolf', 'Uruloki', 'Blessed Realm', 'Nen Girith', 'Beleriand King Thingol', 'Teiglin Turin', 'Denethor', 'Winged Crown', 'Seven Stars', 'Eldalie', 'Fingon Fingolfin', 'Trees Galadriel', 'VALAQUENTA', 'Gimilzor', 'Between Himring', 'Elvenhome', 'Dreadful Death', 'Vanyar']\n",
      "\n",
      "Further classified NE length: 1349\n",
      "\n",
      "Only NE length: 1129\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFurther classified NE: \")\n",
    "print(list(ne_entities_f)[:50])\n",
    "\n",
    "print(\"Only NE: \")\n",
    "print(list(ne_entities)[:50])\n",
    "\n",
    "print(\"\\nFurther classified NE length: \"+ str(len(list(ne_entities_f))))\n",
    "print(\"\\nOnly NE length: \"+ str(len(list(ne_entities))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./../res/entities_binary.json\", \"w\")\n",
    "f.write(json.dumps(ne_entities))\n",
    "f.close()\n",
    "\n",
    "f = open(\"./../res/entities_all.json\", \"w\")\n",
    "f.write(json.dumps(ne_entities_f))\n",
    "f.close()\n",
    "\n",
    "#print(ne_entities_f)\n",
    "## shows choosen entity types -> they are mostly incorrect anyway, this I will keep using binary chunked NE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with custom patterns\n",
    "\n",
    "I tried to choose the named entities in a way that would be close to the style of writing, since the text consists of an amount of nouns hyphenated per preposition e.g. Eye of Sauron, Grey Havens of Lindon, Towers of Mist, ...\n",
    "Yet, many of the found entities are either nouns followed by a misinterpreted verb e.g. 'Eldar desire', 'Manwe wept' or 'Middle-earth lay' or nouns mixed with misinterpreted prepositions such as therefore or thereafter.\n",
    "\n",
    "Overall I found the custom-found entities better in terms of complete definition (e.g. Seeing Stone of Emyn Beraid, Feanor son of Finwe, Finduilas daughter of Orodreth, Lord of Morgul) but of course too complex wikipedia styled page heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\"\"\"\n",
    "Function takes tagged tokens and grammar as input and prints the number of such found entities and first 15 of them\n",
    "\"\"\"\n",
    "def printCustomPattern(tagged_tokens, pattern_grammar):\n",
    "    cp = nltk.RegexpParser(pattern_grammar) ## train model for input grammar\n",
    "    res = cp.parse(tagged_tokens) ## apply parser to pos-tagged tokens \n",
    "    ents = extractEntities(res)  ## extract found patterns from results\n",
    "    ents_len = len(ents)\n",
    "    print(\"Found \"+str(ents_len)+\" strings with this pattern.\")\n",
    "    print(\"Some found strings with given pattern: \")\n",
    "    if(ents_len > 15):\n",
    "        print(list(ents)[:15])\n",
    "    else:\n",
    "        print(list(ents))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"All tagged words: \")\n",
    "#print(len(tagged))\n",
    "#print(list(tagged)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chooses named entities as nouns with first upper case letter, adds .. if connected by preposition 'of' i.e. Gates of Argonath.\n",
    "\n",
    "\"\"\"\n",
    "found = []\n",
    "custom = []\n",
    "\n",
    "for word in tagged:\n",
    "    if (word[1].startswith(\"N\") or word[1].startswith(\"FW\") or (found and word[1].startswith(\"IN\") and word[0] in ['of'])):\n",
    "        found.append(word)\n",
    "        #print(word, word[1].startswith(\"NN\"),word[1].startswith(\"FW\"), word[1].startswith(\"IN\"), word[1].startswith(\"TO\"))\n",
    "\n",
    "    else:\n",
    "        if (found) and found[-1][1].startswith(\"IN\"): #remove possible prepositions if in the beginning\n",
    "            #print(\"\\tin \", found[-1][1])\n",
    "            found.pop()\n",
    "        if (found and \" \".join(e[0] for e in found)[0].isupper()): #if starts with upper case\n",
    "            #print(\"\\t\", \" \".join(e[0] for e in found))\n",
    "            custom.append(\" \".join(e[0] for e in found))\n",
    "        found = []\n",
    "        \n",
    "print(set(custom))\n",
    "\n",
    "\n",
    "f = open(\"./../res/entities_custom.json\", \"w\")\n",
    "f.write(json.dumps(list(set(custom))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other experiments with own patterns:\n",
    "Couple experiments with picking different grammar patterns from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 606 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['the Stewards', 'the Doomsman', 'the Halfling', 'all Brandir', 'this World', 'the Balrogs', 'the High', 'the Steadfast', 'the Flame', 'the Gwaith-i-Mirdain', 'the Crissaegrim', 'the Uruloki', 'the Gorgoroth', 'the Calacirya', 'the Ever-young']\n",
      "Found 1799 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['the building', 'this place', 'the duel', 'this doom', 'that counsel', 'the warden', 'that reason', 'a seed', 'a hall', 'each awoke', 'No love', 'the anguish', 'any treasury', 'a loss', 'a friendship']\n"
     ]
    }
   ],
   "source": [
    "grammar1 = \"SP:{<DT><NNP>}\" ## chooses DT (determiner) followed by NNP (proper noun)\n",
    "printCustomPattern(tagged, grammar1)\n",
    "\n",
    "\n",
    "grammar2 = \"SP:{<DT><NN>}\" ## chooses DT (determiner) followed by NN (some noun)\n",
    "printCustomPattern(tagged, grammar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 785 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['a great captain', 'the first hour', 'a great rock', 'the inmost circle', 'the first Sun', 'a dreadful fall', 'the right line', 'the left hand', 'the unsullied Light', 'the mighty ravine', 'any other thing', 'a sudden Nahar', 'the uttermost West', 'the first victory', 'a new star']\n"
     ]
    }
   ],
   "source": [
    "grammar3 = \"SP:{<DT><JJ><NN|NNP>}\" ## chooses sequences of DT (determiner) followed by JJ (adjective) and NN (some or proper noun)\n",
    "printCustomPattern(tagged, grammar3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['house was well-nigh destroyed', 'power took cruel revenge', 'this deed Fingon won great renown', 'Morgoth gave small heed', 'hast found thy brother', 'Morgoth sent great strength', 'Beleriand did great evil', 'Caranthir paid little heed', 'Manwe said unto Melkor', 'the Valar gathered great store', 'tongue had great power', 'Earendil was long time', 'Manwe put forth Morgoth', 'the western world were rent asunder', 'Isil was first wrought']\n"
     ]
    }
   ],
   "source": [
    "grammar4 = \"Sentence: {<DT|PP\\$>*<JJ>*<NN|NNP>+<VBD><JJ><NN|NNP>}\" ## some simple setences \n",
    "printCustomPattern(tagged, grammar4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['Noldor had as little thought of faith', 'axe smoked in the black blood of the troll-guard', 'thereafter surpassed that desperate crossing in hardihood', 'dominion round about with an unseen wail of shadow', 'Pelori was an empty land in twilight', 'Fiercest burned the new flame of desire', 'Amandil set sail in a small ship at night', 'blade rang a cold voice in answer', 'Manwe made a high feast for the praising', 'danger was fraught with dreadful power because of the holy jewel']\n"
     ]
    }
   ],
   "source": [
    "grammar5 = \"Sentence: {<NN|NNP><VBD><JJ>*<NN|NNP>*<DT|IN>+<JJ>+<NN|NNP><DT|IN>+<JJ>*<NN|NP>+}\" ## some more simplish setences \n",
    "printCustomPattern(tagged, grammar5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['victory of the Elves was dear-bought For those of Ossiriand', 'air of Middle-earth became heavy with the breath of growth', 'Huan the hound was true of heart']\n"
     ]
    }
   ],
   "source": [
    "grammar6 = \"Sentence: {<NN|NNP><VBD>*<DT|IN>+<VBD>*<NN|NNP><VBD><JJ>+<DT|IN>+<JJ>*<DT|IN>*<NN|NP>*<DT|IN>*<JJ>*<NN|NNP>}\" ## some more simple setences \n",
    "printCustomPattern(tagged, grammar6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "Active sentences: "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 461 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['Fingon passed over Anfauglith', 'Gwindor gave the sword', 'foam flew like snow', 'Fingon strung an arrow', 'Tlrion was the name', 'wind came out of the east', 'Galdor ruled the house', 'Sapphire was with Elrond', 'Ores loathed the Master', 'Feanor was at the mouth', 'Melkor brooded in the outer', 'land lay under a cloud', 'Mablung set a guard', 'Linaewen was the name', 'Tulkas left the council']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Phrases (no verbs): "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1824 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['of the fate of Elured', 'The love of Finwe', 'in vision from afar', 'Finduilas daughter of Orodreth the King', 'on the city of Armenelos', 'that in the making of Arda', 'friend upon the island of Tol', 'from the bridge of Menegroth', 'towards the Fen of Serech', 'By the ring of Felagund', 'with spilling of blood', 'under the power of Thingol', 'the Tower of Guard', 'the horse of Celegorm', 'feast of the Spring of Arda']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Simple sentences with <b>proper nouns<b>: "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['Fingon passed over Anfauglith', 'Rohirrim aided the Lords', 'Lord arose in Mirkwood', 'Iluvatar permitted the Valar', 'Varda hallowed the Silmarils', 'Beleg departed from Amon', 'Narog rose beneath the Mountains', 'Varda commanded the Moon', 'Ores loathed the Master', 'Tuor came upon an Elf', 'Valar passed over Middle-earth', 'Sauron called the Nazgul', 'Tuor fought with Maeglin', 'Melian was a Maia', 'Fingolfin marched into Mithrim']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Simple sentences <b>with pronouns<b>: "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['he fled from them', 'she sped on before him', 'he made for them', 'he declared that it', 'he led before them', 'they won for themselves', 'he deemed that in him', 'he knew that it', 'he saw as he', 'he feared that it', 'she went with him', 'he lusted for them', 'she marvelled that she', 'they followed after him', 'he spoke of it']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "Simple sentences <b>without pronouns<b>: "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 strings with this pattern.\n",
      "Some found strings with given pattern: \n",
      "['of what lay before', 'Those who used the Nine Rings', 'against all who came in', 'of what passed in the', 'of those who led the Noldor', 'those who saw the', 'all who heard that sound']\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "grammar7 = \"\"\"SP1:{<NN|NNP><VBD><DT|IN>+<NN|NNP>}\n",
    "            \"\"\"\n",
    "\n",
    "grammar8 = \"\"\"SP2: {<NN|NNP>*<JJ>*<DT|IN>+<NN|NNP><DT|IN><JJ>*<NN|NNP>}\"\"\" ## chooses sequences of DT (determiner) followed by JJ (adjective) and NN (some or proper noun)\n",
    "\n",
    "grammar9 = \"\"\"SP1:{<NNP><VBD><DT|IN|RP>+<NNP>}\n",
    "            \"\"\"\n",
    "\n",
    "grammar10 = \"\"\"SP1:{<PRP|WP><VBD><DT|IN|RP>+<PRP|WP>}\n",
    "            \"\"\"\n",
    "\n",
    "grammar11 = \"\"\"SP1:{<DT|IN>+<WP>+<VBD><DT|IN|RP>+<JJ>*<NN|NNP>*}\n",
    "            \"\"\"\n",
    "display(HTML(\"\\nActive sentences: \"))\n",
    "printCustomPattern(tagged, grammar7)\n",
    "display(HTML(\"\\nPhrases (no verbs): \"))\n",
    "printCustomPattern(tagged, grammar8)\n",
    "display(HTML(\"\\nSimple sentences with <b>proper nouns<b>: \"))\n",
    "printCustomPattern(tagged, grammar9)\n",
    "display(HTML(\"\\nSimple sentences <b>with pronouns<b>: \"))\n",
    "printCustomPattern(tagged, grammar10)\n",
    "display(HTML(\"\\nSimple sentences <b>without pronouns<b>: \"))\n",
    "printCustomPattern(tagged, grammar11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom entity classification with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function searches the summary text for appropriate entity description. \n",
    "The first found string with the given pattern is saved and returned as dictionary object.\n",
    "The first verb of the description (typically form of the verb be) is removed.\n",
    "If no such pattern is found, the description_string is set to \"Thing\".\n",
    "\n",
    "Arguments: \n",
    "    entity name string, wikipedia summary string\n",
    "\n",
    "Returns:\n",
    "    dict object { entity_name : description_string }\n",
    "\"\"\"\n",
    "def parseSummary(name,summary):\n",
    "    #print(summary[:100])\n",
    "    phrases = {}\n",
    "    pattern_grammar = \"NP: {<VBD|VBZ|VBP><DT|IN|RP|WP>*<JJ|RB>*<NN|NNP|NNS>+<VBD|VBN|VBP>*<DT|IN|RP|WP|JJ>*<VBD|VBN|VBP>*<DT|IN|RP|WP|JJ>*<NN|NNP|NNS>*}\"\n",
    "    cp = nltk.RegexpParser(pattern_grammar) ## train model for input grammar\n",
    "    tagged_tokens = pos_tag(word_tokenize(summary))\n",
    "    res = cp.parse(tagged_tokens) ## apply parser to pos-tagged tokens of summary\n",
    "    #print(res)\n",
    "    ## PICK first entity named NP and save i to phrases -> then return\n",
    "    for result in res:\n",
    "        if isinstance(result, nltk.tree.Tree):\n",
    "            if(result.label() ==  \"NP\"):\n",
    "                phrases[name] = \" \".join([ent[0] for ent in result.leaves()][1:]) # remove first word (typically is, are)\n",
    "                return phrases\n",
    "    \n",
    "    phrases[name] = \"Thing\" \n",
    "    return phrases #when pattern is not found\n",
    "    \n",
    "\n",
    "def findPage(name):\n",
    "    results = wikipedia.search(name)\n",
    "    try:\n",
    "        page = wikipedia.page(name)\n",
    "        ent = parseSummary(name,page.summary)\n",
    "    except:\n",
    "        #print(\"Page not accessible atm exception.\")\n",
    "        return {name: \"Thing\"}\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girlingreyshirt/.local/lib/python3.5/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/girlingreyshirt/.local/lib/python3.5/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page not accessible atm exception.\n",
      "{\n",
      "   \"Ainur\": \"the immortal spirits\",\n",
      "   \"Forest\": \"a large area dominated by trees\",\n",
      "   \"Morgoth\": \"a character from Tolkien\",\n",
      "   \"Mountain\": \"a large landform\",\n",
      "   \"Namo\": \"Thing\",\n",
      "   \"Nimloth\": \"the name of an Elf-maid\",\n",
      "   \"Sapphire\": \"a precious gemstone\",\n",
      "   \"Seven Stones\": \"a traditional South Asian game\",\n",
      "   \"Teleri\": \"the Elves who are\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For list of named entities searches wikipedia pages for short description.\n",
    "Prints (or stores) the found 'dictionary' as json object.\n",
    "\"\"\"\n",
    "def getCategories(entities_t, store = False, name = './../res/named_entitites_wordbook.txt'):\n",
    "    json_object = {}\n",
    "    for entity in entities_t:\n",
    "        ent = findPage(entity)\n",
    "        json_object[entity] = ent[entity]\n",
    "    \n",
    "    print(json.dumps(json_object, indent=3, sort_keys=True, ensure_ascii=False))\n",
    "    if(store):\n",
    "        with open(name, 'w') as outfile:  \n",
    "            json.dump(json_object, outfile)\n",
    "\n",
    "\n",
    "## Couple tests\n",
    "test = ['Morgoth', 'Mountain', 'Forest', 'Sapphire', 'Nimloth', 'Seven Stones', 'Ainur', 'Namo','Teleri']\n",
    "getCategories(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only NE: \n",
      "['Amon Amarth', 'Nienna', 'Twilight Meres', 'Colon', 'Rivil', 'Finrod Finarfin', 'Hallow', 'Emyn Beraid', 'Darkness', 'Crissaegrim', 'Orthanc', 'Younger', 'Annael', 'Borlad', 'Erech', 'Elder Children', 'Serinde', 'Mount Rerir', 'Silmarien', 'Dimbar', 'Anarion', 'White Telperion', 'Eledhwen', 'Gwindor', 'Dark Years']\n"
     ]
    }
   ],
   "source": [
    "print(\"Only NE: \")\n",
    "print(list(ne_entities)[0:25])\n",
    "\n",
    "## Printing found Wikipedia results for couple of named entities:\n",
    "#entities = list(ne_entities)[30:70]\n",
    "#getCategories(entities)\n",
    "\n",
    "\"\"\"create wordbook from nltk entities and save them to named_entitites_wordbook.txt file\"\"\"\n",
    "getCategories(list(ne_entities)[0:400], True) #250 should be enough for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not always successfull even when the right Wikipedia page is found. For example \"Morgoth\" is described as the \"evil in the world if Middle-earth\" but the first word in this description is of course considered a verb thus after erasing it to get a noun phrase (\"in the world if Middle-earth\") the description no longer makes any sense.\n",
    "\n",
    "Other words (Teleri, Exiles etc.) are simply not described conveniently to be easily parsed. If anything I was actually impressed by how many of the terms were even found on Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"AINU LIN DALE\": \"a list of people\",\n",
      "   \"Abyss\": \"with an oil platform crew\",\n",
      "   \"Ainu\": \"an indigenous people of Japan\",\n",
      "   \"Ainur\": \"the immortal spirits\",\n",
      "   \"Arda\": \"a Turkish professional footballer who\",\n",
      "   \"Astaldo\": \"Thing\",\n",
      "   \"Aule\": \"a fictional character from J. R. R. Tolkien\",\n",
      "   \"Behold\": \"a brand of furniture polish\",\n",
      "   \"Being\": \"the existence of a thing\",\n",
      "   \"Being things\": \"the existence of a thing\",\n",
      "   \"Both\": \"Thing\",\n",
      "   \"Breath of Arda\": \"a fictional character in J. R. R. Tolkien\",\n",
      "   \"Children of Eru\": \"a fictional character in J.R.R\",\n",
      "   \"Children of Iluvatar\": \"the name given\",\n",
      "   \"Children of Iluvatar arise therein\": \"Thing\",\n",
      "   \"Children of Iluvatar hearken\": \"Thing\",\n",
      "   \"Darkness\": \"Thing\",\n",
      "   \"Dead\": \"Thing\",\n",
      "   \"Deeps of Time\": \"the concept of geologic time\",\n",
      "   \"Deeps of Time Melkor hath\": \"Thing\",\n",
      "   \"Deer\": \"the hoofed ruminant mammals\",\n",
      "   \"Dominion of Men\": \"a period in J.R.R\",\n",
      "   \"Doomsman\": \"Thing\",\n",
      "   \"Ea\": \"an American video game company headquartered in Redwood City\",\n",
      "   \"Earth\": \"the third planet from the Sun\",\n",
      "   \"Elbereth\": \"Thing\",\n",
      "   \"Eldalie\": \"a surname\",\n",
      "   \"Eldar\": \"Thing\",\n",
      "   \"Eldarin tongue\": \"a constructed language devised by J. R. R. Tolkien\",\n",
      "   \"Elves\": \"a type of human-shaped supernatural\",\n",
      "   \"Elves call Arda\": \"Thing\",\n",
      "   \"Elvish tongue\": \"Thing\",\n",
      "   \"Eru\": \"a fictional character in J.R.R\",\n",
      "   \"Este\": \"Thing\",\n",
      "   \"Feanturi\": \"Thing\",\n",
      "   \"Fire\": \"the rapid oxidation of a material\",\n",
      "   \"Firmament\": \"the structure above the atmosphere\",\n",
      "   \"Firstborn\": \"the first child born\",\n",
      "   \"Flame\": \"Thing\",\n",
      "   \"Flame Imperishable\": \"aspects of Christian theology\",\n",
      "   \"Followers\": \"Thing\",\n",
      "   \"Giver of Fruits\": \"the seed-bearing structure in\",\n",
      "   \"Great\": \"Thing\",\n",
      "   \"Great Music\": \"an album by American composer\",\n",
      "   \"Great Ones\": \"Thing\",\n",
      "   \"Great Ones array\": \"an electronically scanned array\",\n",
      "   \"Grey\": \"an intermediate color between black\",\n",
      "   \"Holy Ones\": \"Sancti\",\n",
      "   \"Houses\": \"a building\",\n",
      "   \"Iluvatar\": \"a fictional character in J.R.R\",\n",
      "   \"Iluvatar arose\": \"Thing\",\n",
      "   \"Irmo\": \"Thing\",\n",
      "   \"Judgements\": \"the evaluation of evidence\",\n",
      "   \"Kementari\": \"characters in J. R. R. Tolkien\",\n",
      "   \"King\": \"the title given\",\n",
      "   \"Kingdom of Arda\": \"the name given\",\n",
      "   \"Kingdom of Earth\": \"an archaic conception of Earth\",\n",
      "   \"Kings\": \"Thing\",\n",
      "   \"Lady\": \"a term of respect\",\n",
      "   \"Later Ages\": \"relatively little material on the ages\",\n",
      "   \"Long\": \"Thing\",\n",
      "   \"Lord\": \"an appellation for a person\",\n",
      "   \"Lord of Waters\": \"Thing\",\n",
      "   \"Lords\": \"the upper house of the Parliament\",\n",
      "   \"Lorien\": \"Thing\",\n",
      "   \"Maiar\": \"Thing\",\n",
      "   \"Mandos\": \"characters in J. R. R. Tolkien\",\n",
      "   \"Mandos cry\": \"a Canadian film\",\n",
      "   \"Manwe\": \"a fictional character in J. R. R. Tolkien\",\n",
      "   \"Manwe dwells Varda\": \"Thing\",\n",
      "   \"Melkor\": \"a surname of Dutch origin\",\n",
      "   \"Melkor hath\": \"Thing\",\n",
      "   \"Men\": \"Thing\",\n",
      "   \"Middle-earth\": \"the fictional setting of much of British writer J. R. R. Tolkien\",\n",
      "   \"Music\": \"an art form\",\n",
      "   \"Namo\": \"Thing\",\n",
      "   \"Nessa\": \"an American radio\",\n",
      "   \"Nienna\": \"a fictional character in J. R. R. Tolkien\",\n",
      "   \"Noldor\": \"High Elves of the Second Clan\",\n",
      "   \"Nonetheless Ulmo\": \"Thing\",\n",
      "   \"Oiolosse\": \"Thing\",\n",
      "   \"Orome\": \"Thing\",\n",
      "   \"Powers\": \"Thing\",\n",
      "   \"Powers of Arda\": \"Arda\",\n",
      "   \"Queen\": \"Thing\",\n",
      "   \"Queens\": \"the easternmost of the\",\n",
      "   \"Sea\": \"the connected body of salty water\",\n",
      "   \"Secret Fire\": \"aspects of Christian theology\",\n",
      "   \"Stars\": \"Thing\",\n",
      "   \"Sulimo\": \"Thing\",\n",
      "   \"Sun\": \"the star at the center\",\n",
      "   \"Therefore Iluvatar\": \"Arda\",\n",
      "   \"Thus news\": \"information about current events\",\n",
      "   \"Time\": \"the indefinite continued progress of existence\",\n",
      "   \"Too\": \"Thing\",\n",
      "   \"Tulkas\": \"an English singer-songwriter\",\n",
      "   \"Tuneless Halls\": \"Thing\",\n",
      "   \"Ulmo\": \"a fictional character in J. R. R. Tolkien\",\n",
      "   \"Ulmo speaks\": \"Thing\",\n",
      "   \"Ulumuri\": \"a mountain range in eastern Tanzania\",\n",
      "   \"V alar\": \"Thing\",\n",
      "   \"VALAQUENTA Account\": \"a collection of mythopoeic works\",\n",
      "   \"Vaire\": \"a commune in the Doubs department\",\n",
      "   \"Valar\": \"Thing\",\n",
      "   \"Valiant\": \"Thing\",\n",
      "   \"Valier\": \"Thing\",\n",
      "   \"Valimar\": \"an Old High German\",\n",
      "   \"Valinor\": \"a fictional location in J. R. R. Tolkien\",\n",
      "   \"Vana\": \"Thing\",\n",
      "   \"Varda\": \"a French film director\",\n",
      "   \"Veil of Arda\": \"a monotheistic faith\",\n",
      "   \"Vision\": \"Thing\",\n",
      "   \"Vision of Iluvatar\": \"characters in J. R. R. Tolkien\",\n",
      "   \"Void\": \"Thing\",\n",
      "   \"Water\": \"a transparent\",\n",
      "   \"Weaver\": \"Thing\",\n",
      "   \"West\": \"the opposite direction from east\",\n",
      "   \"Whereas Melkor\": \"Thing\",\n",
      "   \"World\": \"the planet Earth\",\n",
      "   \"Yavanna\": \"a fictional character from J. R. R. Tolkien\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Comparison between nltk Wikipedia entities (ne_entities) and custom extracted entities\n",
    "\"\"\"create wordbook from custom entities and save them to custom.txt file\"\"\"\n",
    "getCategories(list(custom)[0:400], True, './../res/custom.txt') #250 should be enough for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the resulting custom.txt file shows that, as expected, more entities weren't found as a wikipedia topic and thus categorized as \"Thing\". In adittion, the categorization could not correctly assign many made-up terms, especially those regarding non-english grammar rules, i.e. Ainu (singular) as from Ainur (plural), Ea, Eldar, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
