{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úkol č. 1 - část web scraping\n",
    "\n",
    "\n",
    "## Výběr zdroje dat\n",
    "\n",
    "Vybrala jsem si první katalog:\n",
    "\n",
    "  * [Vysoká škola finanční a správní, a.s.](https://is.vsfs.cz/thesis/)\n",
    "  \n",
    "## Postup stahování a ukládání\n",
    "V proměnné data jsou uloženy paramety vyhledávání.\n",
    "Stahuji každý typ práce zvlášť vždy v cyklu podle počtu stránek jednotlivých prací.\n",
    "Většinu údajů čtu rovnou ze stránky vyhledávání, pouze jména oponenta a vedoucího hledám až na samotné stránce každé práce.\n",
    "Údaje ukládám vždy po 50 do jednotlivých csv souborů (složka CSV a dále složky BC, ING a MGR). Captchu jsem zadávala ručně\n",
    "když bylo potřeba, cyklus requestu jsem na tyto chvíle zastavila input() polem.\n",
    "Po načtení dostatečného množství dat, jsem z jednotlivých csv souborů vytvořila jeden, který obsahuje všechna data, se kterými dále pracuji v druhé a třet části úkolu.\n",
    "\n",
    "CSV soubor je zde: ./CSV/data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerHTML(element):\n",
    "    \"\"\"Returns the inner HTML of an element as a UTF-8 encoded bytestring\"\"\"\n",
    "    return element.encode_contents().decode(\"utf-8\")\n",
    "\n",
    "def getLink(object):\n",
    "    try:\n",
    "        link = object.findAll('a')[0].get('href')\n",
    "        return link\n",
    "    except:\n",
    "        return -1\n",
    "        \n",
    "\n",
    "url = 'https://is.vsfs.cz/thesis/?'\n",
    "url_page = 'https://is.vsfs.cz'\n",
    "\n",
    "data = {\n",
    "    'FAK':'6410',\n",
    "    'PRI' : '-',\n",
    "    'ROK' : '-',\n",
    "    'TIT' : '-', \n",
    "    'PRA' : '-',\n",
    "    'vypsat':'1',\n",
    "    'exppar' : '1',\n",
    "    'por' : '0',\n",
    "}\n",
    "students = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def returnWork(student = object, page  = object, prace = object):\n",
    "    work = {\n",
    "        \"schoolName\": np.nan,\n",
    "        \"studentName\": np.nan,\n",
    "        \"degree\": np.nan,\n",
    "        \"year\" : np.nan,\n",
    "        \"defended\": np.nan,\n",
    "        \"type\":np.nan,\n",
    "        \"program\": np.nan,\n",
    "        \"major\": np.nan,\n",
    "        \"title\": np.nan,\n",
    "        \"supervisor\": np.nan,\n",
    "        \"oponent\": np.nan\n",
    "    }\n",
    "\n",
    "    divisions = page.findAll('div', { \"class\" : \"oddil\"})\n",
    "    \n",
    "    work['studentName'] = innerHTML(student.findAll('b')[0])\n",
    "    if(innerHTML(student.findAll('i')[0]).find('roz.') != -1):\n",
    "        index_i = 1\n",
    "    else:\n",
    "        index_i = 0\n",
    "        \n",
    "    work['schoolName'] = innerHTML(student.findAll('i')[index_i])\n",
    "\n",
    "    icka = student.findAll('i')\n",
    "    \n",
    "    if(is_number(innerHTML(icka[index_i+1]))):\n",
    "        work['year'] = int(innerHTML(icka[index_i+1]))\n",
    "    if(innerHTML(icka[index_i+2]) == \"úspěšně absolvováno\"):\n",
    "        work['defended'] = 1\n",
    "        work['degree'] = innerHTML(icka[index_i+3])\n",
    "    else:\n",
    "        work['defended'] = 0\n",
    "    \n",
    "    work['program'] = innerHTML(icka[index_i+4])\n",
    "    work['major'] = innerHTML(icka[index_i+5])\n",
    "    work['type'] = prace\n",
    "    if(len(icka) > 7+index_i):\n",
    "        work['title'] = innerHTML(icka[-1])\n",
    "    else:\n",
    "        work['title'] = innerHTML(icka[-1])\n",
    "    \n",
    "    if(len(icka) < 7):\n",
    "        work['title'] = np.nan\n",
    "        \n",
    "    \n",
    "    \n",
    "    #display(divisions)\n",
    "    if(len(divisions) > 2):\n",
    "        lines = divisions[2].find_all('li')\n",
    "        headers = divisions[2].find_all('h4')\n",
    "        #display(lines)\n",
    "        if(len(headers) > 1 and innerHTML(headers[-2]) == \"Vedoucí:\"):\n",
    "            string = innerHTML(lines[-2]).split(',')\n",
    "            work['supervisor'] = \",\".join(string[:-1])\n",
    "        if(len(headers) > 1 and innerHTML(headers[-1]) == \"Oponent:\"):\n",
    "            string = innerHTML(lines[-1]).split(',')\n",
    "            work['oponent'] = \",\".join(string[:-1])\n",
    "\n",
    "    return work\n",
    "\n",
    "theses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data BC\n",
    "prace = 'Bc.'\n",
    "\n",
    "for pages in range(1351, 2402, 50):\n",
    "    data['TIT'] = prace\n",
    "    data['ROK'] ='-'\n",
    "    data['por'] = pages\n",
    "    display(data)\n",
    "    r = requests.post(url, data)\n",
    "    r.encoding='utf-8'\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    divs = soup.findAll('div', style=\"margin-left:50px\")\n",
    "    index = 0\n",
    "    deg = soup.new_tag(\"i\")\n",
    "\n",
    "    for div in divs:\n",
    "        if(getLink(div) == -1):\n",
    "            continue\n",
    "        l = requests.post(url_page+getLink(div))\n",
    "        l.encoding='utf-8'\n",
    "        soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        while(l.status_code != 200):\n",
    "            print('max limit!')\n",
    "            print(url_page+getLink(div))\n",
    "            input()\n",
    "            l = requests.post(url_page+getLink(div))\n",
    "            l.encoding='utf-8'\n",
    "            soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        theses.append(returnWork(div, soup_page, prace))\n",
    "    data_50 = pd.DataFrame(theses)\n",
    "    data_50.to_csv(\"./CSV/BC/data_{}.csv\".format(pages),sep=';')\n",
    "    theses.clear()\n",
    "\n",
    "print(\"done Bc\")\n",
    "display(len(theses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data MGR\n",
    "prace = 'Mgr.'\n",
    "\n",
    "for pages in range(1, 51, 50):\n",
    "    data['TIT'] = prace\n",
    "    data['ROK'] ='-'\n",
    "    data['por'] = pages\n",
    "    display(data)\n",
    "    r = requests.post(url, data)\n",
    "    r.encoding='utf-8'\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    divs = soup.findAll('div', style=\"margin-left:50px\")\n",
    "    index = 0\n",
    "    deg = soup.new_tag(\"i\")\n",
    "\n",
    "    for div in divs:\n",
    "        if(getLink(div) == -1):\n",
    "            continue\n",
    "        l = requests.post(url_page+getLink(div))\n",
    "        l.encoding='utf-8'\n",
    "        soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        while(l.status_code != 200):\n",
    "            print('max limit!')\n",
    "            print(url_page+getLink(div))\n",
    "            input()\n",
    "            l = requests.post(url_page+getLink(div))\n",
    "            l.encoding='utf-8'\n",
    "            soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        theses.append(returnWork(div, soup_page, prace))\n",
    "    data_50 = pd.DataFrame(theses)\n",
    "    data_50.to_csv(\"./CSV/MGR/data_{}.csv\".format(pages),sep=';')\n",
    "    theses.clear()\n",
    "\n",
    "print(\"done Mgr\")\n",
    "display(len(theses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get data ING\n",
    "prace = 'Ing.'\n",
    "\n",
    "for pages in range(1101, 2002, 50):\n",
    "    data['TIT'] = prace\n",
    "    data['ROK'] ='-'\n",
    "    data['por'] = pages\n",
    "    display(data)\n",
    "    r = requests.post(url, data)\n",
    "    r.encoding='utf-8'\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    divs = soup.findAll('div', style=\"margin-left:50px\")\n",
    "    index = 0\n",
    "    deg = soup.new_tag(\"i\")\n",
    "\n",
    "    for div in divs:\n",
    "        if(getLink(div) == -1):\n",
    "            continue\n",
    "        l = requests.post(url_page+getLink(div))\n",
    "        l.encoding='utf-8'\n",
    "        soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        while(l.status_code != 200):\n",
    "            print('max limit!')\n",
    "            print(url_page+getLink(div))\n",
    "            input()\n",
    "            l = requests.post(url_page+getLink(div))\n",
    "            l.encoding='utf-8'\n",
    "            soup_page = BeautifulSoup(l.text, \"html.parser\")\n",
    "        theses.append(returnWork(div, soup_page, prace))\n",
    "    data_50 = pd.DataFrame(theses)\n",
    "    data_50.to_csv(\"./CSV/ING/data_{}.csv\".format(pages),sep=';')\n",
    "    theses.clear()\n",
    "\n",
    "print(\"done Ing\")\n",
    "display(len(theses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat data into one CSV file\n",
    "import glob\n",
    "\n",
    "filesBC = glob.glob('CSV/BC/*.csv')\n",
    "filesING = glob.glob('./CSV/ING/*.csv')\n",
    "filesMGR = glob.glob('./CSV/MGR/*.csv')\n",
    "filesBC\n",
    "dfBC =pd.concat([pd.read_csv(f, sep=';') for f in filesBC], ignore_index=True)\n",
    "\n",
    "dfING =pd.concat([pd.read_csv(f, sep=';') for f in filesING], ignore_index=True)\n",
    "dfMGR =pd.concat([pd.read_csv(f, sep=';') for f in filesMGR], ignore_index=True)\n",
    "df = pd.concat([dfBC,dfMGR,dfING], ignore_index=True)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "#save file\n",
    "df.to_csv(\"./CSV/data.csv\",sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9da297d614bb2667dfb21fadcf07f5f64679004c9ae9ad9a1b5a03e6f7f19cbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
